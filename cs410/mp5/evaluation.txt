1. I think that the 10-fold cross validation will give a higher accuracy because this way 10 samples will be used as validation data versus just 5 and averaging more samples always leads to a more accurate estimate. This will allow for the system to have more data to analyze. For a 10-fold validation, there will be 9 samples and then 1 sample used as test data versus in the 5-fold validation where there are only 4 samples with 1 being used as test data. This overall makes the system smarter.

2. Information retrieval evaluation has a lot to do with what documents are relevant and non relevant, with respect to user information need. It is relevant if the information stated is important to the user, not based off a query. On the other hand, text categorization evaluation is determining the clustering of text documents, or placing them in certain classes. Therefore, information retrieval focuses on relevancy while text categorization focuses on classification. One basis it on specific information and another basis it off topics that the documents are in.

3. The confusion matrix will show what the system predicted versus what the actual outcome was so we can read this matrix in order to see where the classifier did mistake one class for another. The numbers outside of the diagonal will show us the errors and the numbers on the diagonal show where the system predicted the data correctly. Example languages from the ICNALE dataset that were confused were “CHN and HKG” and “CHN and TWN”.  

4.The confusion matrix will help to show class imbalances through the percent error we see for each of the languages. For example, in the chart shown, Japanese has much less percentage error than Chinese and this shows that there was more data in the text document for Japanese than Chinese. Therefore, we know there is an imbalance in the data. 